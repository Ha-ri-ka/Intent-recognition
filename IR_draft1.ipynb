{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers\n",
    "# pip install seqeval\n",
    "# pip install pandas\n",
    "# pip install numpy\n",
    "# pip install matplotlib\n",
    "# pip install seaborn\n",
    "# !pip install scikit-learn\n",
    "# !pip install torch torchvision\n",
    "# !pip install nltk\n",
    "# !pip install  spacy\n",
    "# !pip install scipy\n",
    "# !pip install yellowbrick\n",
    "# !pip install tqdm\n",
    "# !pip install openpyxl\n",
    "# !pip install nlpaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Please add a 'Book' node.</td>\n",
       "      <td>add node</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Make a new node with the label 'Movie'.</td>\n",
       "      <td>add node</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I would like to add a node labeled 'Game'.</td>\n",
       "      <td>add node</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Can you create a node with the label 'Animal'?</td>\n",
       "      <td>add node</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I need a new node with the label 'Plant'.</td>\n",
       "      <td>add node</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         sentence    intent\n",
       "0                       Please add a 'Book' node.  add node\n",
       "1         Make a new node with the label 'Movie'.  add node\n",
       "2      I would like to add a node labeled 'Game'.  add node\n",
       "3  Can you create a node with the label 'Animal'?  add node\n",
       "4       I need a new node with the label 'Plant'.  add node"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path=\"final data.xlsx\"\n",
    "data=pd.read_excel(path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA AUGMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BELOW CODE WAS RUN ON GOOGLE COLAB AND COPIED TO HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Please add a \\'Book\\' node.\\n[\"Persons_wishing relegate a \\' Book \\' 4Gbit_s_Fibre_Channel.\"]\\nMake a new node with the label \\'Movie\\'.\\n[\"Reinforce a new node withtheir from Crosstown_Rebels \\' Movie \\'.\"]\\nI would like to add a node labeled \\'Game\\'.\\n[\"\\'ve dared Hmm. to add a node labeled \\' Playoff_Photos \\'.\"]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import nlpaug.augmenter.word as naw\n",
    "# aug=naw.WordEmbsAug(model_type='word2vec',model_path=\"/content/supporting models/word2vec/GoogleNews-vectors-negative300.bin\",action='substitute')\n",
    "# count=0\n",
    "# for sent in data['sentence']:\n",
    "#   print(sent)\n",
    "#   print(aug.augment(sent))\n",
    "#   count+=1\n",
    "#   if count==3:\n",
    "#     break\n",
    "\n",
    "# output:\n",
    "'''Please add a 'Book' node.\n",
    "[\"Persons_wishing relegate a ' Book ' 4Gbit_s_Fibre_Channel.\"]\n",
    "Make a new node with the label 'Movie'.\n",
    "[\"Reinforce a new node withtheir from Crosstown_Rebels ' Movie '.\"]\n",
    "I would like to add a node labeled 'Game'.\n",
    "[\"'ve dared Hmm. to add a node labeled ' Playoff_Photos '.\"]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Please add a 'Book' node.\\nthen contact other'book'node.\\nMake a new node with the label 'Movie'.\\nhas one new album with the name'movie '.\\nI would like to add a node labeled 'Game'.\\nsome would like too add one section labeled'game '.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bertAug=naw.ContextualWordEmbsAug(model_path='bert-base-uncased',action='substitute')\n",
    "# count=0\n",
    "# for sent in data['sentence']:\n",
    "#   print(sent)\n",
    "#   print(bertAug.augment(sent)[0])\n",
    "#   count+=1\n",
    "#   if count==3:\n",
    "#     break\n",
    "  \n",
    "#output\n",
    "'''Please add a 'Book' node.\n",
    "then contact other'book'node.\n",
    "Make a new node with the label 'Movie'.\n",
    "has one new album with the name'movie '.\n",
    "I would like to add a node labeled 'Game'.\n",
    "some would like too add one section labeled'game '.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "augmentation is not working i am giving up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As BERT is trained on a corpus of full sentences of natural language and the embedding process involves contextual embedding, performing stop-word removal might interfere with the accurate functioning of BERT. So we do not remove stop words from our input. This is also why we will not be performing stemming or lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "removing all punctuation marks except '' and \"\" as the specifics of graph manipulation are expected to be input in quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def removePunctuation(sentence):\n",
    "    cleaned=\"\".join([char for char in sentence if char not in string.punctuation or char==\"'\" or char=='\"'])    \n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**tokenization using bert tokenizer** <br>\n",
    "components: <br>\n",
    "-->tokenization and add special tokens thatr BERT requires and then add padding <br>\n",
    "--> attention mask:1 for actual token, 0 for pad tokens<br>\n",
    "-->convert tikens to ids: after you do step 1, you convert each token to unique ID<br>\n",
    "-->each token is converted into a vector of (seqlen x 768) dimension <br>\n",
    "--> feed the attention mask and token IDs to the model after converting them to tensors and unsqueezing them at 0th index ??? <br>\n",
    "--> op= model(tokenIds,attention_mask) <br>\n",
    "the op is the final embdded vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hugging face's BERT tokenizer uses pre trained BERT base, a small version of BERT. we use cased BERT because we want our model to be case sensitive, which will be important for paramter extraction in the named entity recognition layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PRE-PROCESSING FOR BERT** <br>\n",
    "1. [CLS] and [SEP] are special tokens that BERT requires. this is because BERT was roginally trained for Q-A tasks. For classfn tasks, BERT requirws another special token, which is all in-built in the hugging face tokenizer. \n",
    "2. All sequences must be of the same length, so we need to do padding. We do the padding on the right side which is preferred for BERT.\n",
    "3. Finally we pass in attention mask to assign importance to each token. So all non-padding tokens have an attnetion of 1 and oadding tokens have 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "def preProcess(sentence):\n",
    "    tokenizer=BertTokenizer.from_pretrained('bert-base-cased')\n",
    "    encoding=tokenizer.encode_plus(\n",
    "        sentence,\n",
    "        add_special_tokens= True,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=False,\n",
    "        return_tensors='pt' #pt signifies pyTorch, use tf for tensorFlow & keras\n",
    "    )\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex=\"this is a sentence please encode it\"\n",
    "processed=preProcess(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL ARCHITECTURE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
